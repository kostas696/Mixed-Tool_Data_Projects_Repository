# -*- coding: utf-8 -*-
"""absenteeism_module

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YrgzNaT4FVG5U49zpWl8kuyi_MNWLsJc
"""

import pandas as pd
import numpy as np
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

class AbsenteeismPredictor():

    def __init__(self, model_path='model_with_scaler'):
        # Load the trained model and scaler
        with open(model_path, 'rb') as model_file:
            self.model, self.scaler = pickle.load(model_file)
            self.data = None
            self.preprocessed_data = pd.DataFrame()

    # Take a data file (*.csv) and preprocess it in the same way as in the original process
    def preprocess_data(self, input_data):
        # Ensure input_data is a DataFrame with the same format as the original data
        # Apply the same preprocessing steps as before

        # Import the data
        df = pd.read_csv(input_data, delimiter=',')

        # Store the data in a new variable for later use
        self.df_with_predictions = df.copy()

        # Rename columns
        df.columns = df.columns.str.replace(' ', '_')

        # Drop the 'ID' column
        df.drop('ID', axis=1, inplace=True)

        # To preserve the code we've created in the previous section, add a column with 'NaN' strings
        df['Absenteeism_Time_in_Hours'] = 'NaN'

        # Create dummy variables for 'Reason for Absence'
        reason_dummies = pd.get_dummies(df['Reason_for_Absence'], drop_first=True)

        # Group the dummy variables into specified groups
        group1 = reason_dummies.iloc[:, :14].max(axis=1)
        group2 = reason_dummies.iloc[:, 14:17].max(axis=1)
        group3 = reason_dummies.iloc[:, 17:21].max(axis=1)
        group4 = reason_dummies.iloc[:, 21:].max(axis=1)

        # Add the new dummy variables to the DataFrame
        df = pd.concat([df, group1, group2, group3, group4], axis=1)

        # Drop the 'Reason for Absence' column
        df.drop('Reason_for_Absence', axis=1, inplace=True)

        # Rename the new columns
        df.columns = df.columns[:-4].tolist() + ['Group_1', 'Group_2', 'Group_3', 'Group_4']

        column_names = ['Date', 'Transportation_Expense', 'Distance_to_Work', 'Age',
                        'Daily_Work_Load_Average', 'Body_Mass_Index', 'Education', 'Children',
                        'Pets', 'Absenteeism_Time_in_Hours', 'Group_1', 'Group_2', 'Group_3', 'Group_4']
        df.columns = column_names

        # Re-order the columns in df
        column_names_reordered = ['Group_1', 'Group_2', 'Group_3', 'Group_4', 'Date', 'Transportation_Expense', 'Distance_to_Work', 'Age',
                                   'Daily_Work_Load_Average', 'Body_Mass_Index', 'Education', 'Children', 'Pets', 'Absenteeism_Time_in_Hours']

        df = df[column_names_reordered]

        # Convert 'Date' column to datetime format
        df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')

        # Extract month and day of the week
        df['Month'] = df['Date'].dt.month
        df['Day_of_Week'] = df['Date'].dt.dayofweek

        # Drop the 'Date' column
        df.drop('Date', axis=1, inplace=True)

        # Re-order the columns in df
        column_names_upd = ['Group_1', 'Group_2', 'Group_3', 'Group_4', 'Month', 'Day_of_Week',
                             'Transportation_Expense', 'Distance_to_Work', 'Age',
                             'Daily_Work_Load_Average', 'Body_Mass_Index', 'Education', 'Children',
                             'Pets', 'Absenteeism_Time_in_Hours']

        df = df[column_names_upd]

        # Turn the data from the 'Education' column into binary data
        df['Education'] = df['Education'].map({1: 0, 2: 1, 3: 1, 4: 1})

        # Replace the NaN values
        df = df.fillna(value=0)

        # Drop the original absenteeism time
        df = df.drop(['Absenteeism_Time_in_Hours'], axis=1)

        # Drop the variables we decide we don't need
        df = df.drop(['Daily_Work_Load_Average'], axis=1)

        # Update df_with_predictions with the preprocessed data
        self.df_with_predictions = df.copy()

        # Define columns to scale
        columns_to_scale = ['Month', 'Day_of_Week', 'Transportation_Expense', 'Distance_to_Work', 'Age',
                             'Body_Mass_Index', 'Children', 'Pets']

        # Scale only the specified columns
        df[columns_to_scale] = self.scaler.transform(df[columns_to_scale])

        # We have included this line of code if you want to call the 'preprocessed data'
        self.preprocessed_data = df.copy()

        # Set self.data to the scaled data
        self.data = df.copy()

    # A function which outputs the probability of a data point to be 1
    def predicted_probability(self):
        if self.data is not None:
            pred = self.model.predict_proba(self.data)[:, 1]
            return pred

    # A function which outputs 0 or 1 based on our model
    def predicted_output_category(self):
        if self.data is not None:
            pred_outputs = self.model.predict(self.data)
            return pred_outputs

    # Predict the outputs and the probabilities and
    # Add columns with these values at the end of the new data
    def predicted_outputs(self):
        if self.data is not None:
            # Create a new DataFrame with original unscaled columns
            result_df = self.df_with_predictions.copy()

            # Add columns for predictions and probabilities
            result_df['Probability'] = self.model.predict_proba(self.data)[:, 1]
            result_df['Prediction'] = self.model.predict(self.data)

            return result_df